{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71221e14-f1f4-42a8-b2cc-6fa783722155",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40155f4e-9a5c-442b-89c7-b30fc80f8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "from selenium import webdriver \n",
    "import time\n",
    "from selenium.webdriver.common.by import By \n",
    "import pandas as pd \n",
    "import random \n",
    "import json \n",
    "from collections import defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a957a5b-5bcb-40c5-888f-101c6dcbc3a1",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22eafff-94d3-4a69-bf29-dafa07343e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage json \n",
    "def write_json(data,fname,fpath=None):\n",
    "    if fpath:\n",
    "        loc = fpath + '/' + fname + '.json'\n",
    "    else: \n",
    "        loc = os.getcwd()+\"/\" + fname + '.json'\n",
    "\n",
    "    with open(loc, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "\n",
    "def read_json(fname,fpath=None):\n",
    "    if fpath:\n",
    "        loc = fpath + '/' + fname + '.json'\n",
    "    else: \n",
    "        loc = os.getcwd()+\"/\" + fname + '.json'\n",
    "\n",
    "    with open(loc, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2a54e8-fd08-4d19-8a96-364f32aa0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions scrolls till the end of the page\n",
    "\n",
    "def scroll():\n",
    "    last_height = driver.execute_script(\"window.scrollTo(0,100);\")\n",
    "    \n",
    "    while True:\n",
    "        last_height = driver.execute_script(\"return window.scrollY\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "        driver.execute_script(\"window.scrollBy(0,window.scrollY);\")\n",
    "        new_height = driver.execute_script(\"return window.scrollY\")\n",
    "        # print(f'new height = {new_height} , last_height = {last_height}')\n",
    "    \n",
    "        if new_height == last_height:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f06f7633-1749-421e-9217-b9a33ec579aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function collects data from the tiles and accumulates in the tiles list\n",
    "\n",
    "def collect_data(tile_class):\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content,'lxml')\n",
    "    tile = soup.find_all('div',attrs={\"class\":tile_class})\n",
    "    return tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e782a80-6676-4b58-a554-ee210ac065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random wait \n",
    "\n",
    "def wait():\n",
    "    wait = random.randint(1,5)\n",
    "    time.sleep(wait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80404d5-ce22-4c42-958b-80396c3527b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the next button on naurki website and clicks it \n",
    "\n",
    "def click_next(button_class):\n",
    "    buttons = driver.find_elements(By.CLASS_NAME,button_class)\n",
    "\n",
    "    if not buttons:\n",
    "        return 'stop'\n",
    "\n",
    "    next_button_found = 0\n",
    "    for button in buttons:\n",
    "        # print(button.text.lower())\n",
    "        if 'next' in button.text.lower():\n",
    "            next_button_found =1 \n",
    "\n",
    "            is_disabled = button.get_attribute(\"disabled\")\n",
    "            if is_disabled:\n",
    "                return 'stop'\n",
    "            else:\n",
    "                driver.execute_script(\"arguments[0].click()\",button)\n",
    "\n",
    "    if not next_button_found:\n",
    "        return 'stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bc4068-ad00-4823-876e-71ce95877ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text file to bs4 element\n",
    "\n",
    "def file_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read lines and strip newline characters\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23bd4a9-3930-4387-afef-5c4077f08383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract element data and convert it into a dictionary through which the data will be extracted via keys \n",
    "\n",
    "def extract_tag_data_to_dict_and_df(ele):\n",
    "    ele_data = defaultdict(list)\n",
    "\n",
    "    current_ele = ele\n",
    "    c = '0'\n",
    "    while current_ele is not None:\n",
    "        # Safely extract attributes and set defaults\n",
    "        c = current_ele.get('class', c) if hasattr(current_ele, 'get') else c\n",
    "        tag = current_ele.name if hasattr(current_ele, 'name') else '0'\n",
    "        \n",
    "        # Create the key tuple\n",
    "        key = (str(c),str(tag), 'text')\n",
    "\n",
    "        # Get the text, defaulting to empty string if None\n",
    "        text = current_ele.text if current_ele.text is not None else ''\n",
    "        \n",
    "        # Append the text to the list for the corresponding key\n",
    "        ele_data[key].append(text)\n",
    "\n",
    "        # Move to the next element\n",
    "        current_ele = current_ele.next_element\n",
    "\n",
    "    # Remove duplicates from the lists \n",
    "    for k, v in ele_data.items():\n",
    "        ele_data[k] = list(dict.fromkeys(v))\n",
    "\n",
    "    # Create a string representation of the text lists, joined by '--'\n",
    "    ele_data_str = {k: \"--\".join(str(item) for item in v if item) for k, v in ele_data.items()}\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    ele_data_df = pd.DataFrame.from_dict(ele_data_str, orient='index', columns=['text'])\n",
    "\n",
    "    return ele_data_str, ele_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39435bdc-3914-4bf2-a78b-45a98b69be79",
   "metadata": {},
   "source": [
    "## initialise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab07e262-b411-45f5-8cd5-b39c6279facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting url\n",
    "urls = read_json('urls')\n",
    "\n",
    "# current directory\n",
    "cwd = os.getcwd()+\"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863cce8-df94-4d79-8523-c3940b7b280a",
   "metadata": {},
   "source": [
    "## open chrome and get jobs tiles from the url "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526857d5-86ec-45e2-9a7e-1b781fdb3926",
   "metadata": {},
   "source": [
    "### get the data - job_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f345e79-7321-4948-aae0-cf447f36ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "time.sleep(60)\n",
    "\n",
    "# now you enter you main account credentials.  \n",
    "\n",
    "tiles_db = {}\n",
    "\n",
    "\n",
    "for url_id, url in urls.items():\n",
    "    driver.get(url)\n",
    "    wait()\n",
    "    state = 'start'    \n",
    "    tiles = []\n",
    "    while state!='stop':\n",
    "        scroll()\n",
    "        wait()\n",
    "        tile = collect_data(tile_class='srp-jobtuple-wrapper')\n",
    "        if tile:\n",
    "            tiles = tiles + tile\n",
    "        wait()\n",
    "        state = click_next('styles_btn-secondary__2AsIP')\n",
    "        \n",
    "    t_count = 0\n",
    "    for tile in tiles:        \n",
    "        key = url_id+'--tile_'+str(t_count)\n",
    "        tiles_db[key] = str(tile)\n",
    "        t_count+=1\n",
    "\n",
    "driver.close()\n",
    "\n",
    "write_json(tiles_db,'job_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494a409-37de-4b88-9105-f0775b72bcd8",
   "metadata": {},
   "source": [
    "### create df - job_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f4bcb47-e6e7-46db-981d-a887eebd2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the job html file\n",
    "job_1 = read_json('job_1')\n",
    "\n",
    "#converting job elements tags\n",
    "for tile_id, tile in job_1.items():\n",
    "    job_1[tile_id] = BeautifulSoup(tile,'lxml').find('div')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0dd4d50-9993-4cc5-bab1-bd0ec8b97b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_1 = pd.DataFrame(columns=['job_id','company_name','job_title','job_url'])\n",
    "\n",
    "for tile_id,tile in job_1.items():\n",
    "    # extract the data from the tiles\n",
    "    \n",
    "    job_id = tile.get('data-job-id')\n",
    "    title = tile.find('a',class_='title').text.strip()\n",
    "    href = tile.find('a',class_='title')['href']\n",
    "    company = tile.find('a',class_='comp-name').text.strip()\n",
    "\n",
    "    # write the data in the dataframe\n",
    "\n",
    "    data = {\n",
    "        'job_id' : str(job_id),\n",
    "        'company_name' : company,\n",
    "        'job_title' : title,\n",
    "        'job_url' : href\n",
    "    }\n",
    "\n",
    "    df_job_1.loc[len(df_job_1)] = data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0629b4-aaca-4ed8-8dbf-cae9f63c06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the a columns showcasing the duplicates in a dataframe\n",
    "\n",
    "df_job_1['tile_duplicates'] = df_job_1.groupby(df_job_1.columns.tolist()).transform('size')\n",
    "df_job_1 = df_job_1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab70bb58-0ae0-4301-9309-b785e9f639d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_job_1.to_excel(cwd+'job.xlsx',index=False)\n",
    "df_job_1.to_csv(cwd+'job_1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e84d8-ac79-4a65-94a8-8ac1822b2b97",
   "metadata": {},
   "source": [
    "## open chrome and get additional information for each jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ae115-15e0-49bd-bfd1-5776c3d6fe94",
   "metadata": {},
   "source": [
    "### get the data - job_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "497991d1-5440-4b7b-927a-20e9b0f790e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_1 = pd.read_csv(cwd+'job_1.csv')\n",
    "df_job_1['job_id'] = df_job_1['job_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c8dbbbb-c232-42b4-9eb5-ddbb1b84cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "time.sleep(60)\n",
    "\n",
    "# now you enter you dummy account credentials.  \n",
    "\n",
    "tiles = {}\n",
    "\n",
    "for index,job_row in df_job_1.iterrows():\n",
    "    job_id = str(job_row['job_id'])\n",
    "    job_url = job_row['job_url']\n",
    "    \n",
    "    driver.get(job_url)\n",
    "    wait()\n",
    "    scroll()\n",
    "    wait()\n",
    "    tile = collect_data(tile_class='styles_left-section-container__btAcB')\n",
    "    if tile:\n",
    "        tiles[job_id] = str(tile[0])\n",
    "    # tiles = tiles + [(job_id,tile[0])]\n",
    "    wait()\n",
    "        \n",
    "\n",
    "driver.close()\n",
    "\n",
    "# writing the tiles result in a file\n",
    "with open(cwd+'job_2.json', 'w') as json_file:\n",
    "    json.dump(tiles, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f470b34-bc8d-45a9-9997-6c11122385ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c327c66d-1485-40d2-8e98-8c7b51221674",
   "metadata": {},
   "source": [
    "### create df - job_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b905289b-acdf-4473-bbe1-ed37dacc20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the job html file\n",
    "with open(cwd + 'job_2.json', 'r') as json_file:\n",
    "    job_2 = json.load(json_file)\n",
    "\n",
    "# changing the types of elements(tile) corresponding to job id in json dictionary\n",
    "for job_id,tile in job_2.items():\n",
    "    job_2[job_id] = BeautifulSoup(tile,'lxml').find('div')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69365c6c-3086-474e-800e-87f1c379387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a dataframe to find_out columns keys\n",
    "# tile = job_2['150125006493']\n",
    "# dict_tile, df_tile = extract_tag_data_to_dict_and_df(tile)\n",
    "# df_tile.to_csv(cwd+'df_tile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bee4526-cf97-4230-897f-51f16ae7027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of columns_name and \"keys to lookup in dictionary of tile\" \n",
    "\n",
    "column_find_via = {\n",
    "    # 'job_title': (\"['styles_jhc__jd-top-head__MFoZl']\", 'header', 'text'),\n",
    "    # 'company_name':\t(\"['styles_jd-header-comp-name__MvqAI']\", 'a', 'text'),\n",
    "    'company_rating' : \t(\"['styles_amb-rating__4UyFL']\", 'span', 'text'),\n",
    "    'job_experience' : \t(\"['styles_jhc__exp__k_giM']\", 'div', 'text'),\n",
    "    'job_location':\t(\"['styles_jhc__loc___Du2H']\", 'div', 'text'),\n",
    "    'salary': (\"['ni-icon-salary']\", 'span', 'text'),\n",
    "    'keywords':\t(\"['styles_chip__7YCfG', 'styles_clickable__dUW8S']\", 'span', 'text'),\n",
    "    'apply_on_company_site': (\"['styles_company-site-button__C_2YK', 'company-site-button']\", 'button', 'text'),\n",
    "    'easy_apply':(\"['styles_apply-button__uJI3A', 'apply-button']\", 'button', 'text')\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dbc2de7-e0dd-44c9-ba1c-b74684d7a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe for job_2\n",
    "\n",
    "df_job_2 = pd.DataFrame(columns=['job_id']+list(column_find_via.keys()))\n",
    "\n",
    "# populating dataframe for job_2\n",
    "\n",
    "for job_id,tile in job_2.items():\n",
    "    data = {\n",
    "        'job_id':str(job_id)\n",
    "    }\n",
    "    \n",
    "    dict_tile, df_tile = extract_tag_data_to_dict_and_df(tile)\n",
    "    for col in column_find_via:\n",
    "        data[col] = dict_tile.get(column_find_via[col])\n",
    "        \n",
    "    df_job_2.loc[len(df_job_2)] = data\n",
    "\n",
    "df_job_2 = df_job_1.merge(df_job_2,on='job_id',how='left')   \n",
    "\n",
    "# df_job_2.to_excel(cwd+'job.xlsx',index=False)\n",
    "df_job_2.to_csv(cwd+'job_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de4e85c-8ae3-421d-9501-5ecd3e401192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering jobs\n",
    "\n",
    "df_job_3 = pd.read_csv(cwd+'job_2.csv')\n",
    "\n",
    "## write filtering criteria here\n",
    "\n",
    "df_job_3.to_csv(cwd+'job_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b62256-2871-490a-9d9a-bd493f6b19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe\n",
    "\n",
    "df_job_3 = pd.read_csv(cwd+'job_3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "545c5f89-6dca-4c58-868f-83a7cbf2d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_terms = ['analyst','analytics','azure','data','visualization','databricks','pyspark']\n",
    "\n",
    "def calculate_score(keywords):\n",
    "    score = 0\n",
    "    if pd.isna(keywords):\n",
    "        return 1.5\n",
    "    for k in keyword_terms:\n",
    "        score += keywords.lower().count(k.lower())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f09f2866-c01a-46e0-812e-65423dc10fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3['keyword_score'] = df_job_3['keywords'].apply(calculate_score)\n",
    "df_job_3 = df_job_3.sort_values(by=['keyword_score', 'company_rating','easy_apply','company_name'], ascending=[False, False,False,True])\n",
    "df_job_3.to_csv(cwd+'job_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b93c61af-014c-41b3-8679-35b0f9b06214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create html\n",
    "\n",
    "def generate_html_with_tiles(url_dict, filename=f'{cwd}naukri.html'):\n",
    "    # Begin writing the HTML content\n",
    "    html_content = '''<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>URL Tiles</title>\n",
    "    <style>\n",
    "        .tile {\n",
    "            display: inline-block;\n",
    "            margin: 10px;\n",
    "            padding: 20px;\n",
    "            border: 1px solid #ccc;\n",
    "            border-radius: 8px;\n",
    "            background-color: #f9f9f9;\n",
    "            cursor: pointer;\n",
    "            text-align: center;\n",
    "            min-width: 100px;\n",
    "        }\n",
    "        .clicked {\n",
    "            background-color: lightgreen;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"tiles-container\">\n",
    "'''\n",
    "\n",
    "    # Add tiles for each URL\n",
    "    for display_text, url in url_dict.items():\n",
    "        html_content += f'''\n",
    "        <div class=\"tile\" onclick=\"handleClick(this, '{url}')\">\n",
    "            {display_text}\n",
    "        </div>\n",
    "'''\n",
    "\n",
    "    # Add script to handle clicks and storage\n",
    "    html_content += '''\n",
    "    </div>\n",
    "    <script>\n",
    "        function handleClick(tile, url) {\n",
    "            let clickedTiles = JSON.parse(localStorage.getItem('clickedTiles')) || [];\n",
    "            if (clickedTiles.includes(url)) {\n",
    "                // If already clicked, remove from clickedTiles and remove clicked class\n",
    "                clickedTiles = clickedTiles.filter(item => item !== url);\n",
    "                tile.classList.remove('clicked');\n",
    "            } else {\n",
    "                // If not clicked, add to clickedTiles and add clicked class\n",
    "                clickedTiles.push(url);\n",
    "                tile.classList.add('clicked');\n",
    "            }\n",
    "            localStorage.setItem('clickedTiles', JSON.stringify(clickedTiles));\n",
    "            window.open(url, '_blank');  // Open the URL in a new tab\n",
    "        }\n",
    "\n",
    "        // Mark previously clicked tiles\n",
    "        window.onload = function() {\n",
    "            let clickedTiles = JSON.parse(localStorage.getItem('clickedTiles')) || [];\n",
    "            let tiles = document.querySelectorAll('.tile');\n",
    "            tiles.forEach(tile => {\n",
    "                let url = tile.getAttribute('onclick').split(\"handleClick(this, '\")[1].slice(0, -2);\n",
    "                if (clickedTiles.includes(url)) {\n",
    "                    tile.classList.add('clicked');\n",
    "                }\n",
    "            });\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>'''\n",
    "\n",
    "    # Write to the specified HTML file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b261c471-834a-4cc6-a5a2-6751bc92e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3['display_text'] = df_job_3['job_title'].str.cat(df_job_3['company_name'], sep=' | ',na_rep='NA').str.cat(df_job_3['company_rating'].astype(str), sep=' | ',na_rep = 'NA').str.cat(df_job_3['job_location'],sep=' | ',na_rep = 'NA')\n",
    "\n",
    "job_3 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60fe02ca-8b8f-43e7-8b6d-1fb4f51ae7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df_job_3.iterrows():\n",
    "    job_3[row['display_text']] = row['job_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38ace715-bf42-4e36-9509-fee7e832e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_html_with_tiles(job_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab76086-a81f-46b3-8038-df52934e2de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
